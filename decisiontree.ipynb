{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, e\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interviewee = pd.read_csv('interviewee.csv',header=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankstest=pd.read_csv('banks-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:: 14\n",
      "Dataset shape:: (14, 5)\n",
      "Dataset\n"
     ]
    }
   ],
   "source": [
    "banks=pd.read_csv('banks.csv')\n",
    "\n",
    "print(\"Dataset length::\", len(Interviewee))\n",
    "print(\"Dataset shape::\",Interviewee.shape)\n",
    "print(\"Dataset\")\n",
    "#Interviewee.head()\n",
    "#Interviewee.info()\n",
    "#Interviewee.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bankstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_function(classlabel):\n",
    "    classes,classcounts = np.unique(classlabel,return_counts = True)\n",
    "    x=range(len(classes))\n",
    "    for i in x:\n",
    "        c=classcounts[i]/np.sum(classcounts)\n",
    "        entropy = np.sum([(-c)*np.log2(c)])\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40977637753840185\n"
     ]
    }
   ],
   "source": [
    "print(entropy_function(Interviewee.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain_function(data,attribute,classlabel): \n",
    "    \n",
    "    values,counts = np.unique(data[attribute], return_counts=True)\n",
    "    data_entropy = entropy_function(data[label])   \n",
    "\n",
    "    weighted_feature_entropy = np.sum([(counts[i]/np.sum(counts))*entropy_function(data.where(data[attribute]\n",
    "                              ==values[i]).dropna()[classlabel]) for i in range(len(values))])    \n",
    "    feature_info_gain = data_entropy - weighted_feature_entropy\n",
    "      #  print(\"Information gain\"+str(info_gain))\n",
    "    return feature_info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(dataset,features,label) :\n",
    "        item_values=[information_gain_function(dataset,feature,label) for feature in features]\n",
    "         #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        return  best_feature\n",
    "\n",
    "def create_decision_tree(dataset,df,features,label,parent):\n",
    "    #print(\"1\")\n",
    "    d=np.unique(df[label],return_counts=True)\n",
    "    unique_data=np.unique(dataset[label])\n",
    "    \n",
    "    if len(unique_data)<=1:\n",
    "        #print(\"2\")\n",
    "        return unique_data[0]\n",
    "        \n",
    "    \n",
    "    elif len(dataset)==0:\n",
    "        #print(\"3\")\n",
    "        return unique_data[np.argmax(d[1])]\n",
    "       \n",
    "    \n",
    "    elif len(features)==0:\n",
    "        #print(\"4\")\n",
    "        return parent\n",
    "    \n",
    "    else:\n",
    "        #print(\"6\")\n",
    "        parent=unique_data[np.argmax(d[1])]\n",
    "        optimum_feature = best_split(dataset,features,label)\n",
    "        \n",
    "        decision_tree = {optimum_feature:{}}\n",
    "        for value in np.unique(dataset[optimum_feature]):\n",
    "           \n",
    "            min_data=dataset.where(dataset[optimum_feature]==value).dropna()\n",
    "        \n",
    "            min_tree=create_decision_tree(min_data,df,features,label,parent)\n",
    "        \n",
    "        decision_tree[optimum_feature][value]=min_tree\n",
    "        #print(\"5\")\n",
    "        return decision_tree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x_dict,decisionTree,default =1):\n",
    "    for key in list(x_dict.keys()):\n",
    "        if key in list(decisionTree.keys()):\n",
    "            try:\n",
    "                result = decisionTree[key][x_dict[key]]\n",
    "                if isinstance(result, dict):\n",
    "                    return get_prediction(x_dict,result)\n",
    "                else:\n",
    "                    return result\n",
    "            except:\n",
    "                    return default\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(data,decision_tree):\n",
    "    x_dict = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predictions = pd.DataFrame(columns=[\"Predictions\"])\n",
    "    for i in range(len(data)):\n",
    "        predictions.loc[i,\"Predictions\"] = get_prediction(x_dict[i], create_decision_tree, 1.0)\n",
    "\n",
    "    print('The prediction accuracy is: ',(np.sum(predictions[\"Predictions\"] == data[\"label\"])/len(data))*100,'%')\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the features and label\n",
    "features = Interviewee.columns[:-1]\n",
    "label = 'label'\n",
    "parent=None\n",
    "df=Interviewee\n",
    "features\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Interviewee)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree = create_decision_tree(banks, banks, banks.columns[:-1].tolist(),'label',parent)\n",
    "predict(Interviewee,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
